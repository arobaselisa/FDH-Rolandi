{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Names, Scenes and Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import collections\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_range(path, attribute):\n",
    "    ''' Defining the ranges of the first and second pages '''\n",
    "    # Original image\n",
    "    shape = cv2.imread(path,0).shape\n",
    "    \n",
    "    # Range of x_0\n",
    "    x_0_lower = shape[1]*dict_attribute[attribute]['page_0_lower']\n",
    "    x_0_upper = shape[1]*dict_attribute[attribute]['page_0_upper']\n",
    "    # Range of x_1\n",
    "    x_1_lower = shape[1]*dict_attribute[attribute]['page_1_lower'] \n",
    "    x_1_upper = shape[1]*dict_attribute[attribute]['page_1_upper']  \n",
    "\n",
    "    return x_0_lower, x_0_upper, x_1_lower, x_1_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_x(coord, x_0_lower, x_0_upper, x_1_lower, x_1_upper):  \n",
    "    ''' Returns the page where the word is\n",
    "        0 in the first page\n",
    "        1 in the second page\n",
    "        -1 if not in the range \n",
    "    '''\n",
    "    if coord >= x_0_lower and coord <= x_0_upper:\n",
    "        return 0\n",
    "    elif coord >= x_1_lower and coord <= x_1_upper:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bounds(text, dict_bounds, mask, x_0_lower, x_0_upper, x_1_lower, x_1_upper, attribute):\n",
    "    ''' Finds the names and bounds in the image '''\n",
    "    proba = np.array([])\n",
    "    \n",
    "    # Defining the portion of the height of the box we want to keep\n",
    "    ratio_y = int((text['Bottom_Right_Y'] - text['Top_Left_Y'])*dict_attribute[attribute]['width_box'])\n",
    "    # Defining the portion of the width of the box we want to keep\n",
    "    ratio_x = int((text['Bottom_Right_X'] - text['Top_Left_X'])*dict_attribute[attribute]['height_box'])\n",
    "    \n",
    "    # Going through every pixel of the reduced box\n",
    "    for y in range(text['Top_Left_Y'] + ratio_y, text['Bottom_Right_Y'] - ratio_y):\n",
    "        for x in range(text['Top_Left_X'] + ratio_x, text['Bottom_Right_X'] - ratio_x):\n",
    "            # Find their associated probability of being a name\n",
    "            proba = np.append(proba, mask[y][x])\n",
    "            \n",
    "    # Finding the mean probability of being the corresponding attribute for all the pixels in the reduced box        \n",
    "    mean = proba.mean()\n",
    "    if mean > dict_attribute[attribute]['mean_proba_threshold']:\n",
    "        # Depending on coord_x, append extracted text and bounds on page 0 (left) or 1 (right) \n",
    "        coord_x = change_x(text['Top_Left_X'], x_0_lower, x_0_upper, x_1_lower, x_1_upper)\n",
    "        if coord_x != -1:\n",
    "            if coord_x in dict_bounds:\n",
    "                dict_bounds[coord_x].append((text['Top_Left_Y'], attribute, text['Text']))\n",
    "            else:    \n",
    "                dict_bounds[coord_x] = [(text['Top_Left_Y'], attribute, text['Text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes_one_image(page):\n",
    "    ''' Returns the attributes and bounds in one image '''\n",
    "    \n",
    "    # Data from segmentation\n",
    "    segmentation_path = \"./data/Antigone/1_Segmentation_results/\" + page + \".npy\"\n",
    "    data = np.load(segmentation_path)\n",
    "    \n",
    "    dict_bounds = dict()\n",
    "    for i, attribute in enumerate(dict_attribute.keys()):\n",
    "        # Create x ranges\n",
    "        x_0_lower, x_0_upper, x_1_lower, x_1_upper = define_range(\"./data/Antigone/0_Images/\" + page + \".jpg\", attribute)\n",
    "        \n",
    "        # Threshold for attributes segmentation\n",
    "        mask = np.where(data[i+1]>dict_attribute[attribute]['ocr_proba_threshold'],1,0).astype(np.uint8)\n",
    "\n",
    "        # Load results from OCR\n",
    "        image_df = pd.read_csv('./data/Antigone/2_OCR_results/annotations_' + page + '.csv', index_col=0)\n",
    "\n",
    "        # Find the attributes and bounds\n",
    "        image_df.apply(lambda row: find_bounds(row, dict_bounds, mask, x_0_lower, x_0_upper, x_1_lower, x_1_upper, attribute), axis=1)\n",
    "\n",
    "    return dict_bounds\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_dict(dictionnary):\n",
    "    ''' Returns ordered dictionnary of bounds per pages and per coordinates '''\n",
    "    for pages in dictionnary.values():\n",
    "        for ind in [0,1]:\n",
    "            if ind in pages.keys():\n",
    "                pages[ind].sort(key=lambda x: x[0])\n",
    "    return sorted(dictionnary.items(), key = lambda kv:(int(kv[0][1:]), kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_in_json(dictionnary, path):\n",
    "    ''' Saves ordered dictionnary of bounds per pages and per coordinates in a json file'''\n",
    "    with open(path, \"w\") as outfile:  \n",
    "        json.dump(dictionnary, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_in_dict(path):\n",
    "    ''' Loads ordered dictionnary of bounds per pages and per coordinates from a json file'''\n",
    "    with open(path) as json_file: \n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define attributes to extract in images as well as their parameters\n",
    "dict_attribute = {'Name': {'page_0_lower': 0, \n",
    "                            'page_0_upper':4/10, \n",
    "                            'page_1_lower':4/10, \n",
    "                            'page_1_upper':7/10,\n",
    "                            'width_box': 0.4,\n",
    "                            'height_box':0.4,\n",
    "                            'ocr_proba_threshold':0.2, \n",
    "                            'mean_proba_threshold':0.7},\n",
    "                   'Scene': {'page_0_lower': 2/10, \n",
    "                             'page_0_upper':4/10, \n",
    "                             'page_1_lower':6/10, \n",
    "                             'page_1_upper':8/10, \n",
    "                             'width_box': 0.4,\n",
    "                             'height_box':0.4,\n",
    "                             'ocr_proba_threshold':0.2, \n",
    "                             'mean_proba_threshold':0.7},\n",
    "                   'Description': {'page_0_lower': 0, \n",
    "                                   'page_0_upper':1/2, \n",
    "                                   'page_1_lower':1/2, \n",
    "                                   'page_1_upper':1,\n",
    "                                   'width_box': 0,\n",
    "                                   'height_box':0,\n",
    "                                   'ocr_proba_threshold':0.1, \n",
    "                                   'mean_proba_threshold':0.5}\n",
    "                  }\n",
    "\n",
    "def find_attributes():\n",
    "    attributes_bounds = []\n",
    "    pages = []\n",
    "    # Going through all the images\n",
    "    for filename in os.listdir(\"./data/Antigone/0_Images/\"):\n",
    "        if filename.endswith(\".jpg\"): \n",
    "            file_without_extension = os.path.splitext(filename)[0]\n",
    "            #print(file_without_extension)\n",
    "            pages.append(file_without_extension)\n",
    "            # Find attribute in the image\n",
    "            dict_bounds = find_attributes_one_image(file_without_extension)\n",
    "            attributes_bounds.append(dict_bounds)\n",
    "            #print(dict_bounds)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    return order_dict(dict(zip(pages, attributes_bounds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnary = find_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_in_json(dictionnary, \"./data/Antigone/2_OCR_results/Antigone.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json_in_dict(\"./data/Antigone/2_OCR_results/Antigone.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_attributes(data):\n",
    "    '''Extract all atributes extracted by OCR in order, whithout storing coordinated or pages'''\n",
    "    elements = np.empty(shape=(0,2))\n",
    "    # for each page of the libretto\n",
    "    for page in range(len(data)):\n",
    "        # for each left and/or right page\n",
    "        for ind in data[page][1].keys():\n",
    "            # extract elements in the order they appear, without storing coordinates or pages\n",
    "            elements = np.concatenate((elements, np.array(data[page][1][ind])[:, 1:]), axis = 0)\n",
    "    return elements\n",
    "\n",
    "def extract_attribute(elements, attribute):\n",
    "    '''Extract elements in order from specific attribute'''\n",
    "    # Extract text from specific attribute\n",
    "    text_list = [row[1] for row in elements if attribute in row[0]]\n",
    "    # Create string\n",
    "    text = \" \".join(text_list)\n",
    "    # Remove digits\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_attributes = extract_all_attributes(data)\n",
    "description = extract_attribute(all_attributes, 'Description')\n",
    "names = extract_attribute(all_attributes, 'Name')\n",
    "scenes = extract_attribute(all_attributes, 'Scene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Padre Il detto Il Il Il Il La Macfiro Inventoree Inventere Sartore Luogo Creonte altri Cre Eur Cre Se AT Alib  Eur Cre Lea Ear Cre Lea Cre Per Da Non delle Se Won fa Node Il La pace man Che E Eur Lea Cre Eur Cre Eur Lea  Alc Cre Eur Alc Cre Ale Lea Eur Cre Eur Cre Eur Per Cre Lea Eur Ear  Lea Eur Lea Enrifce Eur Veggio Erm Ant Ear Erm Ant Eur Erm Eur Erm Ant Eur Ant Eur Ant Erm Eur  Ant Eur Erm Ant Erm Senza Ant Eur Ant Bur Ant Eur Ant Exr Ant Eur Ant Eur Ant Exr Ant Eur Per  Aut Corone Cuflodi Antigona Scenda Alc Erm EurGià Cre Ant Cre  Ant Erm Eur Ant Cre Ant Erm Ak Lea Cre Ant Cre Ant Cre Ant Cre  Erm Ant Erm parte Lea Alc parte Ant Eur Ant Eur Ant Penfa Alib  Eur Miferi Fine Ale Eur Alc Eur  Erm Alc Erm Eur Erm Alc Erm Eur Alc Erm Cre Lea Alc Eur Cre Erm Cre Erm Cre Lea Eur Alc Erm  Lea Cre Erm Cre Lea Ale Forfe Eur Ant Cre Eur Cre Lea Ant Alc Cre  Io Eur Ant Eur Ant Eur Deggio Ant Eur Ant Lea Forfe Alc Alib Sento Lea Più Sen  Ant Ale Lea Alc Lea Alc  Eur Creon Eur Ajc Ant Alc Ant Crean Eur Creon Ant Più Alc Eur Creon Si Eur Creon Eur Aut Creon Sem  Ant Ad Eur Learco di Tempio Lear La Creon Eur nia Erm Alc Per Eur Ant Erm Creon Ele Creon fo Erm Creon Erm Nurai fe Deb Oggi Tanto Or Separarfi Placatevi Lo fdegno Per  Lear Ant Alc Creon Ant Creon Và Ant De Panitor Lucido Per cui Or Vigor Numi Eur a poner Si và Erm Ant A voi Arbitre Mori Creen Eur Ant Lear Alc Creon Erm Creon Móra Eur Lear Crcon Alle Il Favella  Ant Creon Aut Eur Ce Ant Cre Per Eur Cre Aut Peri Cre Alc Erm  Exr Pietà Cre Eur Cre Eur Che Son Cre Furore  Sopra Fine Erm Lea Eur Erm Eur Ma Erm Eur Che Alik  Per Lea Erm Lea Erm Lea Se Erm Se Lea Che Se Alib  Luogo Ant Learco Lea Ant Della Cre altrove Eur Erm Lea Cre Eur Cre Eur Cre Ant Cre Eur Cre Eur  Cre Perfidi ad Lea Scender Ear Ant Eur Sdegnata Ant So Eur Però Aut Alc Erm So Lea Ant Eur Ant Alc Ant Alc Ant Eur Ant Alc Aut Alc Ant Alc La Ant Alc Erm Eur Ant Lea Alc Alib Ant Erm Lea Per Alc Ear Vieni Erm Ant Eur Erm Ant Erm Eur Ant Erm a Eur Erm Ant Eur  Erm Ant Erm Eur Erm  Alc Lea Alc Lea con Cr Lean Creon Lear Antigona Ant Eurif Creon SCE  Alc Creon Alc Creon Alc Lear Creon Alc Eurif Lear Già Trembe Erm Lear Ant Erm Aut  Erm Ant Eurif Forfe Lear Alc Eurif Creon Alc Creon Aut Eurif Erm Creon Ant Viva Ale Lear Eurif Creon Lafciami Cre  Ant Vivi IL'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    ''' Remove stopwords in all kind of cases '''\n",
    "    stop_words = stopwords.words('italian')+[word.title() for word in stopwords.words('italian')]+[word.upper() for word in stopwords.words('italian')]\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "    return \" \".join(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Padre detto Macfiro Inventoree Inventere Sartore Luogo Creonte altri Cre Eur Cre AT Alib Eur Cre Lea Ear Cre Lea Cre Won fa Node pace man Eur Lea Cre Eur Cre Eur Lea Alc Cre Eur Alc Cre Ale Lea Eur Cre Eur Cre Eur Cre Lea Eur Ear Lea Eur Lea Enrifce Eur Veggio Erm Ant Ear Erm Ant Eur Erm Eur Erm Ant Eur Ant Eur Ant Erm Eur Ant Eur Erm Ant Erm Senza Ant Eur Ant Bur Ant Eur Ant Exr Ant Eur Ant Eur Ant Exr Ant Eur Aut Corone Cuflodi Antigona Scenda Alc Erm EurGià Cre Ant Cre Ant Erm Eur Ant Cre Ant Erm Ak Lea Cre Ant Cre Ant Cre Ant Cre Erm Ant Erm parte Lea Alc parte Ant Eur Ant Eur Ant Penfa Alib Eur Miferi Fine Ale Eur Alc Eur Erm Alc Erm Eur Erm Alc Erm Eur Alc Erm Cre Lea Alc Eur Cre Erm Cre Erm Cre Lea Eur Alc Erm Lea Cre Erm Cre Lea Ale Forfe Eur Ant Cre Eur Cre Lea Ant Alc Cre Eur Ant Eur Ant Eur Deggio Ant Eur Ant Lea Forfe Alc Alib Sento Lea Sen Ant Ale Lea Alc Lea Alc Eur Creon Eur Ajc Ant Alc Ant Crean Eur Creon Ant Alc Eur Creon Eur Creon Eur Aut Creon Sem Ant Eur Learco Tempio Lear Creon Eur nia Erm Alc Eur Ant Erm Creon Ele Creon fo Erm Creon Erm Nurai fe Deb Oggi Tanto Or Separarfi Placatevi fdegno Lear Ant Alc Creon Ant Creon Và Ant De Panitor Lucido Or Vigor Numi Eur poner và Erm Ant Arbitre Mori Creen Eur Ant Lear Alc Creon Erm Creon Móra Eur Lear Crcon Favella Ant Creon Aut Eur Ce Ant Cre Eur Cre Aut Peri Cre Alc Erm Exr Pietà Cre Eur Cre Eur Son Cre Furore Sopra Fine Erm Lea Eur Erm Eur Erm Eur Alik Lea Erm Lea Erm Lea Erm Lea Alib Luogo Ant Learco Lea Ant Cre altrove Eur Erm Lea Cre Eur Cre Eur Cre Ant Cre Eur Cre Eur Cre Perfidi Lea Scender Ear Ant Eur Sdegnata Ant So Eur Però Aut Alc Erm So Lea Ant Eur Ant Alc Ant Alc Ant Eur Ant Alc Aut Alc Ant Alc Ant Alc Erm Eur Ant Lea Alc Alib Ant Erm Lea Alc Ear Vieni Erm Ant Eur Erm Ant Erm Eur Ant Erm Eur Erm Ant Eur Erm Ant Erm Eur Erm Alc Lea Alc Lea Cr Lean Creon Lear Antigona Ant Eurif Creon SCE Alc Creon Alc Creon Alc Lear Creon Alc Eurif Lear Già Trembe Erm Lear Ant Erm Aut Erm Ant Eurif Forfe Lear Alc Eurif Creon Alc Creon Aut Eurif Erm Creon Ant Viva Ale Lear Eurif Creon Lafciami Cre Ant Vivi'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_segment",
   "language": "python",
   "name": "dh_segment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
