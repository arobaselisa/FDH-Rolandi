{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Names, Scenes and Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import collections\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_range(path, attribute):\n",
    "    ''' Defining the ranges of the first and second pages '''\n",
    "    # Original image\n",
    "    shape = cv2.imread(path,0).shape\n",
    "    \n",
    "    # Range of x_0\n",
    "    x_0_lower = shape[1]*dict_attribute[attribute]['page_0_lower']\n",
    "    x_0_upper = shape[1]*dict_attribute[attribute]['page_0_upper']\n",
    "    # Range of x_1\n",
    "    x_1_lower = shape[1]*dict_attribute[attribute]['page_1_lower'] \n",
    "    x_1_upper = shape[1]*dict_attribute[attribute]['page_1_upper']  \n",
    "\n",
    "    return x_0_lower, x_0_upper, x_1_lower, x_1_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_x(coord, x_0_lower, x_0_upper, x_1_lower, x_1_upper):  \n",
    "    ''' Returns the page where the word is\n",
    "        0 in the first page\n",
    "        1 in the second page\n",
    "        -1 if not in the range \n",
    "    '''\n",
    "    if coord >= x_0_lower and coord <= x_0_upper:\n",
    "        return 0\n",
    "    elif coord >= x_1_lower and coord <= x_1_upper:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bounds(text, dict_bounds, mask, x_0_lower, x_0_upper, x_1_lower, x_1_upper, attribute):\n",
    "    ''' Finds the names and bounds in the image '''\n",
    "    proba = np.array([])\n",
    "    \n",
    "    # Defining the portion of the height of the box we want to keep\n",
    "    ratio_y = int((text['Bottom_Right_Y'] - text['Top_Left_Y'])*dict_attribute[attribute]['width_box'])\n",
    "    # Defining the portion of the width of the box we want to keep\n",
    "    ratio_x = int((text['Bottom_Right_X'] - text['Top_Left_X'])*dict_attribute[attribute]['height_box'])\n",
    "    \n",
    "    # Going through every pixel of the reduced box\n",
    "    for y in range(text['Top_Left_Y'] + ratio_y, text['Bottom_Right_Y'] - ratio_y):\n",
    "        for x in range(text['Top_Left_X'] + ratio_x, text['Bottom_Right_X'] - ratio_x):\n",
    "            # Find their associated probability of being a name\n",
    "            proba = np.append(proba, mask[y][x])\n",
    "            \n",
    "    # Finding the mean probability of being the corresponding attribute for all the pixels in the reduced box        \n",
    "    mean = proba.mean()\n",
    "    if mean > dict_attribute[attribute]['mean_proba_threshold']:\n",
    "        # Depending on coord_x, append extracted text and bounds on page 0 (left) or 1 (right) \n",
    "        coord_x = change_x(text['Top_Left_X'], x_0_lower, x_0_upper, x_1_lower, x_1_upper)\n",
    "        if coord_x != -1:\n",
    "            if coord_x in dict_bounds:\n",
    "                dict_bounds[coord_x].append((text['Top_Left_Y'], attribute, text['Text']))\n",
    "            else:    \n",
    "                dict_bounds[coord_x] = [(text['Top_Left_Y'], attribute, text['Text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes_one_image(page):\n",
    "    ''' Returns the attributes and bounds in one image '''\n",
    "    \n",
    "    # Data from segmentation\n",
    "    segmentation_path = \"./data/Antigone/1_Segmentation_results/\" + page + \".npy\"\n",
    "    data = np.load(segmentation_path)\n",
    "    \n",
    "    dict_bounds = dict()\n",
    "    for i, attribute in enumerate(dict_attribute.keys()):\n",
    "        # Create x ranges\n",
    "        x_0_lower, x_0_upper, x_1_lower, x_1_upper = define_range(\"./data/Antigone/0_Images/\" + page + \".jpg\", attribute)\n",
    "        \n",
    "        # Threshold for attributes segmentation\n",
    "        mask = np.where(data[i+1]>dict_attribute[attribute]['ocr_proba_threshold'],1,0).astype(np.uint8)\n",
    "\n",
    "        # Load results from OCR\n",
    "        image_df = pd.read_csv('./data/Antigone/2_OCR_results/annotations_' + page + '.csv', index_col=0)\n",
    "\n",
    "        # Find the attributes and bounds\n",
    "        image_df.apply(lambda row: find_bounds(row, dict_bounds, mask, x_0_lower, x_0_upper, x_1_lower, x_1_upper, attribute), axis=1)\n",
    "\n",
    "    return dict_bounds\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_dict(dictionnary):\n",
    "    ''' Returns ordered dictionnary of bounds per pages and per coordinates '''\n",
    "    for pages in dictionnary.values():\n",
    "        for ind in [0,1]:\n",
    "            if ind in pages.keys():\n",
    "                pages[ind].sort(key=lambda x: x[0])\n",
    "    return sorted(dictionnary.items(), key = lambda kv:(int(kv[0][1:]), kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_in_json(dictionnary, path):\n",
    "    ''' Saves ordered dictionnary of bounds per pages and per coordinates in a json file'''\n",
    "    with open(path, \"w\") as outfile:  \n",
    "        json.dump(dictionnary, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_in_dict(path):\n",
    "    ''' Loads ordered dictionnary of bounds per pages and per coordinates from a json file'''\n",
    "    with open(path) as json_file: \n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define attributes to extract in images as well as their parameters\n",
    "dict_attribute = {'Name': {'page_0_lower': 0, \n",
    "                            'page_0_upper':4/10, \n",
    "                            'page_1_lower':4/10, \n",
    "                            'page_1_upper':7/10,\n",
    "                            'width_box': 0.4,\n",
    "                            'height_box':0.4,\n",
    "                            'ocr_proba_threshold':0.2, \n",
    "                            'mean_proba_threshold':0.7},\n",
    "                   'Scene': {'page_0_lower': 0, \n",
    "                             'page_0_upper':4/10, \n",
    "                             'page_1_lower':5/10, \n",
    "                             'page_1_upper':8/10, \n",
    "                             'width_box': 0.4,\n",
    "                             'height_box':0.4,\n",
    "                             'ocr_proba_threshold':0.1, \n",
    "                             'mean_proba_threshold':0.7},\n",
    "                   'Description': {'page_0_lower': 0, \n",
    "                                   'page_0_upper':1/2, \n",
    "                                   'page_1_lower':1/2, \n",
    "                                   'page_1_upper':1,\n",
    "                                   'width_box': 0,\n",
    "                                   'height_box':0,\n",
    "                                   'ocr_proba_threshold':0.1, \n",
    "                                   'mean_proba_threshold':0.5}\n",
    "                  }\n",
    "\n",
    "def find_attributes():\n",
    "    attributes_bounds = []\n",
    "    pages = []\n",
    "    # Going through all the images\n",
    "    for filename in os.listdir(\"./data/Antigone/0_Images/\"):\n",
    "        if filename.endswith(\".jpg\"): \n",
    "            file_without_extension = os.path.splitext(filename)[0]\n",
    "            #print(file_without_extension)\n",
    "            pages.append(file_without_extension)\n",
    "            # Find attribute in the image\n",
    "            dict_bounds = find_attributes_one_image(file_without_extension)\n",
    "            attributes_bounds.append(dict_bounds)\n",
    "            #print(dict_bounds)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    return order_dict(dict(zip(pages, attributes_bounds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnary = find_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_in_json(dictionnary, \"./data/Antigone/2_OCR_results/Antigone.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json_in_dict(\"./data/Antigone/2_OCR_results/Antigone.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    ''' Remove stopwords in all kind of cases '''\n",
    "    stop_words = stopwords.words('italian')+[word.title() for word in stopwords.words('italian')]+[word.upper() for word in stopwords.words('italian')]\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "    return tokens_without_sw\n",
    "\n",
    "def extract_all_attributes(data):\n",
    "    '''Extract all atributes extracted by OCR in order, whithout storing coordinated or pages'''\n",
    "    elements = np.empty(shape=(0,2))\n",
    "    # for each page of the libretto\n",
    "    for page in range(len(data)):\n",
    "        # for each left and/or right page\n",
    "        for ind in data[page][1].keys():\n",
    "            # extract elements in the order they appear, without storing coordinates or pages\n",
    "            elements = np.concatenate((elements, np.array(data[page][1][ind])[:, 1:]), axis = 0)\n",
    "    return elements\n",
    "\n",
    "def extract_attribute(elements, attribute):\n",
    "    '''Extract elements in order from specific attribute'''\n",
    "    # Extract text from specific attribute\n",
    "    text_list = [row[1] for row in elements if attribute in row[0]]\n",
    "    # Create string\n",
    "    text = \" \".join(text_list)\n",
    "    # Remove digits\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_attributes = extract_all_attributes(data)\n",
    "description = extract_attribute(all_attributes, 'Description')\n",
    "names = extract_attribute(all_attributes, 'Name')\n",
    "scenes = extract_attribute(all_attributes, 'Scene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ermione': ['Erm', 'dErmione', 'Ermione', 'ErmioneudiftiIl', 'Ermiene'],\n",
       " 'Alcefte': ['Alcefle', 'Alcefie', 'Alcefte', 'Alceie'],\n",
       " 'Creonte': ['Creente', 'Creonte'],\n",
       " 'Learco': ['Learco', 'Learcoe', 'Learce', 'Learto'],\n",
       " 'Autigona': ['Autigo', 'Aut', 'Autigona'],\n",
       " 'Antigona': ['fudettoedAntigont',\n",
       "  'Antigona',\n",
       "  'Ant',\n",
       "  'Antigana',\n",
       "  'dAntigona',\n",
       "  'Antigoua',\n",
       "  'dAntiope'],\n",
       " 'Eurifteo': ['Eurifico',\n",
       "  'Euriftec',\n",
       "  'Eurifteo',\n",
       "  'Euritleo',\n",
       "  'Eur',\n",
       "  'Euriftco',\n",
       "  'Eurifleo']}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = extract_complete_names(names, description)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Autigona': ['fudettoedAntigont',\n",
       "  'Antigona',\n",
       "  'Aut',\n",
       "  'Ant',\n",
       "  'Autigo',\n",
       "  'Autigona',\n",
       "  'Antigana',\n",
       "  'dAntigona',\n",
       "  'Antigoua',\n",
       "  'dAntiope']}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from difflib import SequenceMatcher as sm\n",
    "\n",
    "dic_new = {}\n",
    "names_keys = list(dic.keys())\n",
    "names_values = list(dic.values())\n",
    "\n",
    "for i in range(len(names_keys)):\n",
    "    for j in range(i+1, len(names_keys)):\n",
    "        if(sm(None, names_keys[i], names_keys[j]).ratio() >= 0.60):            \n",
    "            if names_keys[i] in dic_new.keys():\n",
    "                dic_new[names_keys[i]].extend(names_values[i])\n",
    "                dic_new[names_keys[i]].extend(names_values[j])\n",
    "            else:\n",
    "                dic_new[names_keys[i]] = names_values[i]\n",
    "                dic_new[names_keys[i]].extend(names_values[j])\n",
    "#        else:\n",
    "#            if names_keys[i] in dic_new.keys():\n",
    "#                dic_new[names_keys[i]].extend(names_values[i])\n",
    "#            else:\n",
    "#                dic_new[names_keys[i]] = names_values[i]\n",
    "dic_new = {k:list(set(j)) for k,j in dic_new.items()}\n",
    "dic_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scenes_and_acts(attributes_clean):\n",
    "    ''' Adds the acts in the attributes\n",
    "        And stores the scenes as numbers '''\n",
    "    count_scene = 0\n",
    "    count_act = 0\n",
    "    mask = np.ones((np.shape(all_attributes)))\n",
    "    # Goes through all the list\n",
    "    for i, att in enumerate(all_attributes):\n",
    "        # through all the text that has the 'Scene' tag\n",
    "        if (att[0]=='Scene'):\n",
    "            # Remove punctuation and case\n",
    "            word = att[1].lower().translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "            # If a scene is the first one, we add the begining of an act\n",
    "            if (word=='prima'):\n",
    "                count_act += 1\n",
    "                count_scene = 0\n",
    "                attributes_clean[i] = ['Act', count_act]\n",
    "            # Detects the scene, stores its number\n",
    "            elif (word=='scena'):\n",
    "                count_scene += 1\n",
    "                attributes_clean[i] = ['Scene', count_scene]\n",
    "            else:\n",
    "            # Otherwise, we will delete this row\n",
    "                mask[i] = 0\n",
    "    # Delete rows that we don't need anymore\n",
    "    attributes_clean = attributes_clean[mask.astype(np.bool)]\n",
    "    attributes_clean = attributes_clean.reshape(int(np.shape(attributes_clean)[0]/2), 2)\n",
    "    return attributes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_clean = clean_scenes_and_acts(all_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_segment",
   "language": "python",
   "name": "dh_segment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
